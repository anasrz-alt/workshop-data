{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning-Based Obstacle Avoidance Policy\n",
        "\n",
        "## Abstract\n",
        "This experiment demonstrates a simple learning-based obstacle avoidance system for a mobile robot. The system uses synthetic depth sensor data (front, left, right) and expert demonstrations to train a neural network policy that outputs steering and forward commands. The trained policy is evaluated in a simulated environment where it navigates around obstacles.\n"
      ],
      "metadata": {
        "id": "5pbCUA9hRLh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjxVikoLOjNt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Synthetic Dataset Generation\n",
        "The `ObstacleAvoidDataset` class creates a dataset of sensor readings and corresponding expert control commands:\n",
        "- **Inputs:** Depth measurements from front, left, and right directions.\n",
        "- **Targets:** Steering and forward commands generated by an expert rule:\n",
        "  - If an obstacle is close in front, turn toward the side with more space.\n",
        "  - Otherwise, go straight forward.\n",
        "\n",
        "This provides supervised training data for imitation learning."
      ],
      "metadata": {
        "id": "niFk0zuNRRYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic dataset: depth from front, left, right (3 values), and command (steer, forward) from expert\n",
        "class ObstacleAvoidDataset(Dataset):\n",
        "    def __init__(self, n_samples=2000, seed=0):\n",
        "        np.random.seed(seed)\n",
        "        self.depth_front = np.random.uniform(0.5, 5.0, size=(n_samples,))\n",
        "        self.depth_left = np.random.uniform(0.5, 5.0, size=(n_samples,))\n",
        "        self.depth_right = np.random.uniform(0.5, 5.0, size=(n_samples,))\n",
        "        # Expert policy: if front obstacle is close, turn to side with more space\n",
        "        steer = []\n",
        "        forward = []\n",
        "        for f, l, r in zip(self.depth_front, self.depth_left, self.depth_right):\n",
        "            if f < 1.0:\n",
        "                if l > r:\n",
        "                    steer.append(-1.0)  # turn left\n",
        "                else:\n",
        "                    steer.append(1.0)   # turn right\n",
        "                forward.append(0.2)\n",
        "            else:\n",
        "                steer.append(0.0)\n",
        "                forward.append(1.0)\n",
        "        self.inputs = np.stack([self.depth_front, self.depth_left, self.depth_right], axis=1).astype(np.float32)\n",
        "        self.targets = np.stack([steer, forward], axis=1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "dmjOrxwXRQOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural Network Policy\n",
        "The **SimplePolicy** model is a feedforward neural network that learns to map depth inputs to control outputs:\n",
        "- 3 input neurons (depth sensors)\n",
        "- 2 output neurons (steer, forward)\n",
        "- Hidden layers: 64 and 32 neurons with ReLU activations  \n",
        "The network predicts steering angle and forward speed from sensor inputs.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Training and Evaluation\n",
        "The model is trained using **Mean Squared Error (MSE)** loss between predicted and expert commands:\n",
        "- **Optimizer:** Adam (learning rate = 1e-3)\n",
        "- **Metrics:** Root Mean Square Error (RMSE) on test data\n",
        "\n",
        "The training loop iteratively minimizes the imitation loss to replicate expert behavior.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HXPpMXLxRbAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplePolicy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 32), nn.ReLU(),\n",
        "            nn.Linear(32, 2)  # steer, forward\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_policy(policy, loader, optimizer, device):\n",
        "    policy.train()\n",
        "    loss_fn = nn.MSELoss()\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = policy(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test_policy(policy, loader, device):\n",
        "    policy.eval()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = policy(x)\n",
        "            total_loss += ((pred - y)**2).sum().item()\n",
        "            count += y.numel()\n",
        "    return (total_loss / count)**0.5  # RMSE"
      ],
      "metadata": {
        "id": "F1xF8TZXRZdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Simulation\n",
        "After training, the policy is deployed in a simulated 2D environment:\n",
        "- The robot starts at the origin and moves forward.\n",
        "- Depth readings are computed synthetically based on the robot’s position relative to a wall.\n",
        "- The trained policy predicts control actions (steering, forward velocity) at each step.\n",
        "- The robot’s trajectory is updated accordingly.\n",
        "\n",
        "This simulates **real-time obstacle avoidance** behavior based on learned sensor-action mapping.\n"
      ],
      "metadata": {
        "id": "ssBSVv-JRo80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_robot(policy, device, n_steps=50):\n",
        "    # Simulate a robot moving; at each step get synthetic sensor reading, compute action\n",
        "    pos = np.array([0.0, 0.0])\n",
        "    heading = 0.0  # angle, radians\n",
        "    dt = 0.1\n",
        "    path = [pos.copy()]\n",
        "    for _ in range(n_steps):\n",
        "        # fake obstacles: let's say there is a wall at x = 5, so front depth = (5 - x_pos)/cos\n",
        "        depth_front = (5.0 - pos[0]) / max(np.cos(heading), 0.1)\n",
        "        depth_left = (5.0 - pos[0]) / max(np.cos(heading + np.pi/4), 0.1)\n",
        "        depth_right = (5.0 - pos[0]) / max(np.cos(heading - np.pi/4), 0.1)\n",
        "        depth_front = np.clip(depth_front + np.random.randn()*0.1, 0.1, 10.0)\n",
        "        depth_left = np.clip(depth_left + np.random.randn()*0.1, 0.1, 10.0)\n",
        "        depth_right = np.clip(depth_right + np.random.randn()*0.1, 0.1, 10.0)\n",
        "        inp = torch.tensor([[depth_front, depth_left, depth_right]], dtype=torch.float32, device=device)\n",
        "        with torch.no_grad():\n",
        "            out = policy(inp)\n",
        "        steer, forward = out.cpu().numpy()[0]\n",
        "        # update heading\n",
        "        heading += steer * dt\n",
        "        pos += forward * dt * np.array([np.cos(heading), np.sin(heading)])\n",
        "        path.append(pos.copy())\n",
        "    return np.array(path)"
      ],
      "metadata": {
        "id": "awjKa06VRoO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = ObstacleAvoidDataset(n_samples=5000)\n",
        "train_ds, test_ds = torch.utils.data.random_split(dataset, [4000,1000])\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=64)\n",
        "\n",
        "policy = SimplePolicy().to(device)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
        "# Train\n",
        "for epoch in range(10):\n",
        "    train_policy(policy, train_loader, optimizer, device)\n",
        "    rmse = test_policy(policy, test_loader, device)\n",
        "    print(f\"Epoch {epoch} RMSE = {rmse:.4f}\")\n",
        "# Simulate\n",
        "path = simulate_robot(policy, device)\n",
        "print(\"Simulated path:\", path)"
      ],
      "metadata": {
        "id": "5q_2mN8wRxL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Discussion\n",
        "- The trained neural network successfully learns the rule-based obstacle avoidance behavior.\n",
        "- In simulation, the robot slows down and turns when obstacles appear ahead.\n",
        "- The model can be extended to handle more complex scenarios, such as noisy sensors, dynamic obstacles, or continuous control tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Key Concepts\n",
        "- **Imitation Learning:** The policy learns by mimicking an expert’s actions rather than by trial and error.\n",
        "- **Sensor Fusion:** Multiple depth sensors (front, left, right) provide spatial awareness.\n",
        "- **End-to-End Control:** The network directly outputs control commands from raw sensor inputs.\n"
      ],
      "metadata": {
        "id": "bGH2my8bR6kc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mHD8Sr41R7Af"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
