{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning-Based Obstacle Avoidance Policy\n",
        "\n",
        "## Abstract\n",
        "This experiment demonstrates a simple learning-based obstacle avoidance system for a mobile robot. The system uses synthetic depth sensor data (front, left, right) and expert demonstrations to train a neural network policy that outputs steering and forward commands. The trained policy is evaluated in a simulated environment where it navigates around obstacles.\n"
      ],
      "metadata": {
        "id": "5pbCUA9hRLh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DjxVikoLOjNt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Synthetic Dataset Generation\n",
        "The `ObstacleAvoidDataset` class creates a dataset of sensor readings and corresponding expert control commands:\n",
        "- **Inputs:** Depth measurements from front, left, and right directions.\n",
        "- **Targets:** Steering and forward commands generated by an expert rule:\n",
        "  - If an obstacle is close in front, turn toward the side with more space.\n",
        "  - Otherwise, go straight forward.\n",
        "\n",
        "This provides supervised training data for imitation learning."
      ],
      "metadata": {
        "id": "niFk0zuNRRYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic dataset: depth from front, left, right (3 values), and command (steer, forward) from expert\n",
        "class ObstacleAvoidDataset(Dataset):\n",
        "    def __init__(self, n_samples=2000, seed=0):\n",
        "        np.random.seed(seed)\n",
        "        self.depth_front = np.random.uniform(0.5, 5.0, size=(n_samples,))\n",
        "        self.depth_left = np.random.uniform(0.5, 5.0, size=(n_samples,))\n",
        "        self.depth_right = np.random.uniform(0.5, 5.0, size=(n_samples,))\n",
        "        # Expert policy: if front obstacle is close, turn to side with more space\n",
        "        steer = []\n",
        "        forward = []\n",
        "        for f, l, r in zip(self.depth_front, self.depth_left, self.depth_right):\n",
        "            if f < 1.0:\n",
        "                if l > r:\n",
        "                    steer.append(-1.0)  # turn left\n",
        "                else:\n",
        "                    steer.append(1.0)   # turn right\n",
        "                forward.append(0.2)\n",
        "            else:\n",
        "                steer.append(0.0)\n",
        "                forward.append(1.0)\n",
        "        self.inputs = np.stack([self.depth_front, self.depth_left, self.depth_right], axis=1).astype(np.float32)\n",
        "        self.targets = np.stack([steer, forward], axis=1).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "dmjOrxwXRQOU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural Network Policy\n",
        "The **SimplePolicy** model is a feedforward neural network that learns to map depth inputs to control outputs:\n",
        "- 3 input neurons (depth sensors)\n",
        "- 2 output neurons (steer, forward)\n",
        "- Hidden layers: 64 and 32 neurons with ReLU activations  \n",
        "The network predicts steering angle and forward speed from sensor inputs.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Training and Evaluation\n",
        "The model is trained using **Mean Squared Error (MSE)** loss between predicted and expert commands:\n",
        "- **Optimizer:** Adam (learning rate = 1e-3)\n",
        "- **Metrics:** Root Mean Square Error (RMSE) on test data\n",
        "\n",
        "The training loop iteratively minimizes the imitation loss to replicate expert behavior.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HXPpMXLxRbAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimplePolicy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(3, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 32), nn.ReLU(),\n",
        "            nn.Linear(32, 2)  # steer, forward\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_policy(policy, loader, optimizer, device):\n",
        "    policy.train()\n",
        "    loss_fn = nn.MSELoss()\n",
        "    for x, y in loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = policy(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test_policy(policy, loader, device):\n",
        "    policy.eval()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = policy(x)\n",
        "            total_loss += ((pred - y)**2).sum().item()\n",
        "            count += y.numel()\n",
        "    return (total_loss / count)**0.5  # RMSE"
      ],
      "metadata": {
        "id": "F1xF8TZXRZdk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Simulation\n",
        "After training, the policy is deployed in a simulated 2D environment:\n",
        "- The robot starts at the origin and moves forward.\n",
        "- Depth readings are computed synthetically based on the robot’s position relative to a wall.\n",
        "- The trained policy predicts control actions (steering, forward velocity) at each step.\n",
        "- The robot’s trajectory is updated accordingly.\n",
        "\n",
        "This simulates **real-time obstacle avoidance** behavior based on learned sensor-action mapping.\n"
      ],
      "metadata": {
        "id": "ssBSVv-JRo80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_robot(policy, device, n_steps=50):\n",
        "    # Simulate a robot moving; at each step get synthetic sensor reading, compute action\n",
        "    pos = np.array([0.0, 0.0])\n",
        "    heading = 0.0  # angle, radians\n",
        "    dt = 0.1\n",
        "    path = [pos.copy()]\n",
        "    for _ in range(n_steps):\n",
        "        # fake obstacles: let's say there is a wall at x = 5, so front depth = (5 - x_pos)/cos\n",
        "        depth_front = (5.0 - pos[0]) / max(np.cos(heading), 0.1)\n",
        "        depth_left = (5.0 - pos[0]) / max(np.cos(heading + np.pi/4), 0.1)\n",
        "        depth_right = (5.0 - pos[0]) / max(np.cos(heading - np.pi/4), 0.1)\n",
        "        depth_front = np.clip(depth_front + np.random.randn()*0.1, 0.1, 10.0)\n",
        "        depth_left = np.clip(depth_left + np.random.randn()*0.1, 0.1, 10.0)\n",
        "        depth_right = np.clip(depth_right + np.random.randn()*0.1, 0.1, 10.0)\n",
        "        inp = torch.tensor([[depth_front, depth_left, depth_right]], dtype=torch.float32, device=device)\n",
        "        with torch.no_grad():\n",
        "            out = policy(inp)\n",
        "        steer, forward = out.cpu().numpy()[0]\n",
        "        # update heading\n",
        "        heading += steer * dt\n",
        "        pos += forward * dt * np.array([np.cos(heading), np.sin(heading)])\n",
        "        path.append(pos.copy())\n",
        "    return np.array(path)"
      ],
      "metadata": {
        "id": "awjKa06VRoO9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = ObstacleAvoidDataset(n_samples=5000)\n",
        "train_ds, test_ds = torch.utils.data.random_split(dataset, [4000,1000])\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=64)\n",
        "\n",
        "policy = SimplePolicy().to(device)\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
        "# Train\n",
        "for epoch in range(10):\n",
        "    train_policy(policy, train_loader, optimizer, device)\n",
        "    rmse = test_policy(policy, test_loader, device)\n",
        "    print(f\"Epoch {epoch} RMSE = {rmse:.4f}\")\n",
        "# Simulate\n",
        "path = simulate_robot(policy, device)\n",
        "print(\"Simulated path:\", path)"
      ],
      "metadata": {
        "id": "5q_2mN8wRxL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79176f54-2955-4c93-fe98-519bc26eebb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 RMSE = 0.2670\n",
            "Epoch 1 RMSE = 0.2435\n",
            "Epoch 2 RMSE = 0.2239\n",
            "Epoch 3 RMSE = 0.1995\n",
            "Epoch 4 RMSE = 0.1843\n",
            "Epoch 5 RMSE = 0.1728\n",
            "Epoch 6 RMSE = 0.1646\n",
            "Epoch 7 RMSE = 0.1575\n",
            "Epoch 8 RMSE = 0.1499\n",
            "Epoch 9 RMSE = 0.1451\n",
            "Simulated path: [[ 0.00000000e+00  0.00000000e+00]\n",
            " [ 1.11785054e-01  5.98993502e-04]\n",
            " [ 2.22710915e-01  1.75306620e-03]\n",
            " [ 3.34262364e-01  3.42209672e-03]\n",
            " [ 4.45201509e-01  5.52655885e-03]\n",
            " [ 5.53490028e-01  7.92285323e-03]\n",
            " [ 6.62967086e-01  1.06230512e-02]\n",
            " [ 7.70853505e-01  1.35261071e-02]\n",
            " [ 8.78812537e-01  1.66530540e-02]\n",
            " [ 9.85810570e-01  1.98620026e-02]\n",
            " [ 1.09279558e+00  2.32074367e-02]\n",
            " [ 1.19989263e+00  2.65867851e-02]\n",
            " [ 1.30580183e+00  2.99451906e-02]\n",
            " [ 1.41107716e+00  3.32294825e-02]\n",
            " [ 1.51621073e+00  3.63510546e-02]\n",
            " [ 1.62059633e+00  3.93071972e-02]\n",
            " [ 1.72466729e+00  4.20070033e-02]\n",
            " [ 1.82849351e+00  4.44774764e-02]\n",
            " [ 1.93088053e+00  4.66758268e-02]\n",
            " [ 2.03289622e+00  4.86178570e-02]\n",
            " [ 2.13419380e+00  5.02753359e-02]\n",
            " [ 2.23500581e+00  5.15680149e-02]\n",
            " [ 2.33467718e+00  5.24798860e-02]\n",
            " [ 2.43439485e+00  5.29547544e-02]\n",
            " [ 2.53370817e+00  5.29576549e-02]\n",
            " [ 2.63183123e+00  5.24517926e-02]\n",
            " [ 2.73035803e+00  5.13987570e-02]\n",
            " [ 2.82806999e+00  4.97791088e-02]\n",
            " [ 2.92594359e+00  4.74062293e-02]\n",
            " [ 3.02256408e+00  4.43868880e-02]\n",
            " [ 3.11849731e+00  4.06604727e-02]\n",
            " [ 3.21457485e+00  3.62166647e-02]\n",
            " [ 3.31028689e+00  3.10893986e-02]\n",
            " [ 3.40561733e+00  2.53750664e-02]\n",
            " [ 3.50118044e+00  1.89769320e-02]\n",
            " [ 3.59700429e+00  1.21561560e-02]\n",
            " [ 3.69018811e+00  5.36682929e-03]\n",
            " [ 3.78634106e+00 -1.88230763e-03]\n",
            " [ 3.88000547e+00 -9.00956143e-03]\n",
            " [ 3.95940828e+00 -1.52106992e-02]\n",
            " [ 4.03844874e+00 -2.08728690e-02]\n",
            " [ 4.11976631e+00 -2.63983039e-02]\n",
            " [ 4.17891474e+00 -2.99080221e-02]\n",
            " [ 4.22707199e+00 -3.20318699e-02]\n",
            " [ 4.28026764e+00 -3.35380931e-02]\n",
            " [ 4.29980509e+00 -3.35483925e-02]\n",
            " [ 4.35205612e+00 -3.32071484e-02]\n",
            " [ 4.39320876e+00 -3.25576590e-02]\n",
            " [ 4.43964760e+00 -3.15403907e-02]\n",
            " [ 4.47645264e+00 -2.99446897e-02]\n",
            " [ 4.51249849e+00 -2.75290803e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Discussion\n",
        "- The trained neural network successfully learns the rule-based obstacle avoidance behavior.\n",
        "- In simulation, the robot slows down and turns when obstacles appear ahead.\n",
        "- The model can be extended to handle more complex scenarios, such as noisy sensors, dynamic obstacles, or continuous control tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Key Concepts\n",
        "- **Imitation Learning:** The policy learns by mimicking an expert’s actions rather than by trial and error.\n",
        "- **Sensor Fusion:** Multiple depth sensors (front, left, right) provide spatial awareness.\n",
        "- **End-to-End Control:** The network directly outputs control commands from raw sensor inputs.\n"
      ],
      "metadata": {
        "id": "bGH2my8bR6kc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mHD8Sr41R7Af"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}
