{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Theoretical Background  \n",
        "### Adversarial Robustness in Image Classification  \n",
        "Deep neural networks, while highly effective in visual recognition tasks, are vulnerable to **adversarial attacks** ‚Äî carefully crafted perturbations to input data that can cause models to make incorrect predictions. These perturbations are often imperceptible to humans but can dramatically reduce a model‚Äôs performance, exposing critical weaknesses in deep learning systems used in autonomous vehicles, surveillance, and security applications.  \n",
        "\n",
        "One of the most widely studied attack methods is the **Fast Gradient Sign Method (FGSM)**, which generates adversarial examples by adjusting each input pixel in the direction of the gradient of the loss function with respect to the input image. This reveals how sensitive a model is to small but targeted changes in input data, providing valuable insights into its robustness.  "
      ],
      "metadata": {
        "id": "__imUxGCLJAE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fsUZt-cKygP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Problem Statement  \n",
        "The objective of this experiment is to **evaluate the adversarial vulnerability** of a simple convolutional neural network (CNN) designed to distinguish between ‚Äúcar‚Äù and ‚Äúnon-car‚Äù images in a synthetic dataset. The pipeline demonstrates how even a simple vision model trained on controlled data can be deceived by adversarial perturbations.  \n",
        "\n",
        "---\n",
        "\n",
        "## Methodology  \n",
        "### Synthetic Dataset Generation  \n",
        "A custom dataset is created where each 32√ó32 RGB image represents one of two classes:  \n",
        "- **Class 0 (no_car):** Random pixel noise representing background scenes.  \n",
        "- **Class 1 (car):** Images containing a bright rectangular region in the center, simulating the presence of a car.  \n",
        "\n",
        "This dataset allows controlled experimentation without the complexity of real-world images.  \n"
      ],
      "metadata": {
        "id": "mfzFyiqbLcjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Synthetic Dataset ----\n",
        "class SyntheticCarDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Generates simple 32x32 RGB images:\n",
        "    - Class 0 (no_car): random noise.\n",
        "    - Class 1 (car): centered bright rectangle (simulating a 'car' pattern).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_samples=1000, img_size=32, transform=None):\n",
        "        self.n_samples = n_samples\n",
        "        self.img_size = img_size\n",
        "        self.transform = transform\n",
        "\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            label = np.random.randint(0, 2)\n",
        "            img = np.random.rand(3, img_size, img_size) * 0.3  # background noise\n",
        "            if label == 1:\n",
        "                # draw a bright rectangle as \"car\"\n",
        "                start = img_size // 4\n",
        "                end = 3 * img_size // 4\n",
        "                img[:, start:end, start:end] += 0.7  # brighter center\n",
        "            img = np.clip(img, 0, 1)\n",
        "            self.images.append(img.astype(np.float32))\n",
        "            self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.images[idx]\n",
        "        y = self.labels[idx]\n",
        "        x = torch.tensor(x)\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, torch.tensor(y, dtype=torch.long)"
      ],
      "metadata": {
        "id": "QVVtaI3TLVaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture  \n",
        "A lightweight **convolutional neural network (CNN)** is trained for binary classification. The model includes two convolutional layers with ReLU activation, max pooling for spatial downsampling, and a fully connected output layer producing logits for the two classes.  "
      ],
      "metadata": {
        "id": "H9CUuyCJLixi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Model ----\n",
        "class SimpleCarClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * (32 // 4) * (32 // 4), 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "vgHHjM9ALpI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Evaluation  \n",
        "The model is trained using the **Adam optimizer** and **cross-entropy loss** on batches of synthetic images. Training and testing accuracies are monitored to evaluate the classifier‚Äôs performance on clean data.  \n",
        "\n",
        "### Adversarial Attack (FGSM)  \n",
        "After training, the model is subjected to an **FGSM adversarial attack**. For each input image, the attack computes the gradient of the loss with respect to the input and adds a small perturbation (`Œµ`) in the direction of the gradient‚Äôs sign. The perturbed images are then re-evaluated by the model to measure performance degradation.  "
      ],
      "metadata": {
        "id": "petx9ak6L09x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Adversarial Attack (FGSM) ----\n",
        "def adversarial_fgsm(model, x, y, eps=0.02):\n",
        "    \"\"\"\n",
        "    Fast Gradient Sign Method (FGSM) Attack.\n",
        "    \"\"\"\n",
        "    x.requires_grad = True\n",
        "    logits = model(x)\n",
        "    loss = nn.CrossEntropyLoss()(logits, y)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    adv_x = x + eps * x.grad.sign()\n",
        "    return torch.clamp(adv_x, 0.0, 1.0)\n",
        "\n",
        "# ---- Training & Evaluation ----\n",
        "def train_classifier(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = nn.CrossEntropyLoss()(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test_classifier(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        preds = model(x).argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "2DIWCOmiLu8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization  \n",
        "A comparison between original and adversarial examples illustrates how imperceptible changes can cause misclassification. Visualizations provide intuitive understanding of model vulnerability and the importance of robust training strategies.  "
      ],
      "metadata": {
        "id": "KNFR8UqRL6m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Generate synthetic dataset\n",
        "train_ds = SyntheticCarDataset(1500)\n",
        "test_ds = SyntheticCarDataset(500)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=64)\n",
        "\n",
        "model = SimpleCarClassifier().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Train the model\n",
        "print(\"üöó Training synthetic car classifier...\")\n",
        "for epoch in range(5):\n",
        "    train_classifier(model, train_loader, optimizer, device)\n",
        "    acc = test_classifier(model, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1} | Clean test accuracy: {acc:.4f}\")\n",
        "\n",
        "# Run FGSM adversarial attack on one batch\n",
        "x0, y0 = next(iter(test_loader))\n",
        "x0, y0 = x0.to(device), y0.to(device)\n",
        "adv_x = adversarial_fgsm(model, x0, y0, eps=0.03)\n",
        "\n",
        "with torch.no_grad():\n",
        "    orig_preds = model(x0).argmax(dim=1).cpu().numpy()\n",
        "    adv_preds = model(adv_x).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "print(\"\\nOriginal preds:\", orig_preds[:10])\n",
        "print(\"Adversarial preds:\", adv_preds[:10])\n",
        "\n",
        "# Visualize example\n",
        "idx = 0\n",
        "fig, axes = plt.subplots(1, 2, figsize=(5, 3))\n",
        "\n",
        "orig_img = x0[idx].detach().cpu().numpy().transpose(1, 2, 0)\n",
        "adv_img = adv_x[idx].detach().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "axes[0].imshow(orig_img)\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "axes[1].imshow(adv_img)\n",
        "axes[1].set_title(\"Adversarial\")\n",
        "axes[1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9euwv11uMAca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computational Challenge  \n",
        "While this simplified framework highlights the principles of adversarial vulnerability, scaling to real-world autonomous systems presents major challenges. Real vehicle perception models process high-dimensional, dynamic scenes where adversarial attacks could exploit subtle patterns across time and space. Therefore, improving **robustness, generalization, and defense mechanisms** remains a critical direction for research in safe AI-driven perception systems."
      ],
      "metadata": {
        "id": "4Acx_r5PMMBB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_scltCkzMMZg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}